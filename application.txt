
#==================================================== PART 1: index.php [root folder] ===========================================
#================================================================================================================================

1) the first page of this application which is served.
	- There's a `Click to upload file` input tag, through which you can select the file containing URLs [on which you'll be performing further actions].
	- After you select a file, if the input box turns `green` in color, it means you're good to go ahead, and a submit button will be displayed.
	- After you select a file, if the input box turns `red` in color, it means there's some problem at the server end or the network, and the displayed submit button will be disabled.
	- There's a checkbox also below the input box, which provides you the option of `analyzing page's Network`.
	- As soon as the file is selected, two POST requests are sent to the server
		-> first one uploads the file to the server
		-> second one extracts the host name from the file's URLs, sendsit to the server and checks whether a config file exists for the hostname or not, and accordingly sets the `config` flag. 

2) The submit button acts acording to the selections on main page and the `config` flag. 
	- If a config file of the website [whose URLs you are providing through the file] has already been made, it starts scraping the URLs
	- If a config file has not been made, it picks the first URL from the list and opens only that, so config can be made and saved.
	- If the checkbox is selected, it opens the first URL from the list for network traffic analysis i.e. displays the flow of network for that page.

3) As soon as the scrapping starts, the code also keeps sending a request every 10 seconds to check the status of the scrapper from 	server.js.

#==================================================== PART 3: server.js [root folder] ===========================================
#================================================================================================================================

1) This file is the server end of the application, which catches all the requests being made from the webpage of URL we opened. It re writes the URLs present on the page document so that they pass through our server before being sent to the actual website server. It can be started from system console using command `node server.js` and stopped using `Ctrl + C`

2) It also handles the GET/POST requests we send to run the application. When it receives the request to start the scrapping process, it runs the scrapper (scraper.js) [puppeteer headless code] on rtech.

#==================================================== PART 2: scrapper.js [root folder] ===========================================
#================================================================================================================================

1) This script will receive the required arguments from `server.js`, process them, and then run the headless browser and open-scrape the links it receives.

#==================================================== PART 4: script.js [js folder] ============================================
#===============================================================================================================================

1) Present inside the `js` folder, this file is injected in the original webpage's document by `server.js`. It contains the code for scraping part, both for selecting elemnts while creating config, and for autoscraping information based on an already available config.

#==================================================== PART 5: config.js [config folder] ========================================
#===============================================================================================================================

1) consists of config information eg. the ip which'll be used acros application. At present it sets the IP that'll be used across all scripts in the application

#==================================================== PART 6: analyze.html [root folder] =======================================
#===============================================================================================================================

1) This is the page which'll display the request-response information of the URL provided. It contains the code which when loaded on the browser end, will send a request to the server to GET the analysis part collected, and display them on the front end.

#==================================================== PART 7: running the application ===========================================
#================================================================================================================================

1) If there's no `node_modules` folder, but only a `package.json` file, go to project folder via console and run `npm install`

2) To install puppeteer, run command `npm i --save puppeteer` [ https://www.npmjs.com/package/puppeteer ]

3) Go to the project folder and first run the server script using `node server.js`
	
	NOTE: this will start the server temporarirly; to make it run in forever state we have installed `pm2` service [https://www.npmjs.com/package/pm2]
	Some common commands used are:
	- pm2 start <script_name.js> [ will start the process to run server.js ]
	- pm2 list	[ will list all the processes currently running through pm2, along with their index no ]
	- pm2 stop <index_no>	[ will stop the process with specified index no ]
	- pm2 restart <index_no>[ will restart the service of the given index no ]

	NOTE: f you make any changes to the file `server.js`, you'll have to restart it via pm2.

4) You can visit the application at `http://www.auto_scraper.com`

#==================================================== PART 8: headers.js [js folder] ============================================
#================================================================================================================================

1) This file is used in `server.js` file. It includes functions which is used for removing unwanted/invalid characters from response headers.

	NOTE: this code was available online for the above written purpose. There is nothing to be changed in it.

#==================================================== PART 9: Common Git commands ===============================================
#================================================================================================================================
	git status

	to add new files
	git add .
	or
	git add filename

	to commit all added and modified files
	git commit .
	or
	git commit filename

	#########################################
	push all code to git 

	git push origin branchname
	git push origin tripti


	username:- jitendraworking
	Pass:- jitendraworking


	git pull origin tripti